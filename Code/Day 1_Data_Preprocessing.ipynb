{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习100天——第1天：数据预处理（Data Preprocessing）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一步：导入需要的库\n",
    "这两个是我们每次都需要导入的库。NumPy包含数学计算函数。Pandas用于导入和管理数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二步：导入数据集\n",
    "数据集通常是.csv格式。CSV文件以文本形式保存表格数据。文件的每一行是一条数据记录。我们使用Pandas的read_csv方法读取本地csv文件为一个数据帧。然后，从数据帧中制作自变量和因变量的矩阵和向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Importing dataset\n",
      "X\n",
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n",
      "Y\n",
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('../datasets/Data.csv')\n",
    "X = dataset.iloc[ : , :-1].values\n",
    "Y = dataset.iloc[ : , 3].values\n",
    "print(\"Step 2: Importing dataset\")\n",
    "print(\"X\")\n",
    "print(X)\n",
    "print(\"Y\")\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三步：处理丢失数据\n",
    "我们得到的数据很少是完整的。数据可能因为各种原因丢失，为了不降低机器学习模型的性能，需要处理数据。我们可以用整列的平均值或中间值替换丢失的数据。我们用sklearn.preprocessing库中的Imputer类完成这项任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Step 3: Handling the missing data\n",
      "step2\n",
      "X\n",
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values = \"NaN\", strategy = \"mean\", axis = 0)\n",
    "imputer = imputer.fit(X[ : , 1:3])\n",
    "X[ : , 1:3] = imputer.transform(X[ : , 1:3])\n",
    "print(\"---------------------\")\n",
    "print(\"Step 3: Handling the missing data\")\n",
    "print(\"step2\")\n",
    "print(\"X\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四步：解析分类数据\n",
    "分类数据指的是含有标签值而不是数字值的变量。取值范围通常是固定的。例如\"Yes\"和\"No\"不能用于模型的数学计算，所以需要解析成数字。为实现这一功能，我们从sklearn.preprocessing库导入LabelEncoder类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Step 4: Encoding categorical data\n",
      "X\n",
      "[[  1.00000000e+00   0.00000000e+00   0.00000000e+00   4.40000000e+01\n",
      "    7.20000000e+04]\n",
      " [  0.00000000e+00   0.00000000e+00   1.00000000e+00   2.70000000e+01\n",
      "    4.80000000e+04]\n",
      " [  0.00000000e+00   1.00000000e+00   0.00000000e+00   3.00000000e+01\n",
      "    5.40000000e+04]\n",
      " [  0.00000000e+00   0.00000000e+00   1.00000000e+00   3.80000000e+01\n",
      "    6.10000000e+04]\n",
      " [  0.00000000e+00   1.00000000e+00   0.00000000e+00   4.00000000e+01\n",
      "    6.37777778e+04]\n",
      " [  1.00000000e+00   0.00000000e+00   0.00000000e+00   3.50000000e+01\n",
      "    5.80000000e+04]\n",
      " [  0.00000000e+00   0.00000000e+00   1.00000000e+00   3.87777778e+01\n",
      "    5.20000000e+04]\n",
      " [  1.00000000e+00   0.00000000e+00   0.00000000e+00   4.80000000e+01\n",
      "    7.90000000e+04]\n",
      " [  0.00000000e+00   1.00000000e+00   0.00000000e+00   5.00000000e+01\n",
      "    8.30000000e+04]\n",
      " [  1.00000000e+00   0.00000000e+00   0.00000000e+00   3.70000000e+01\n",
      "    6.70000000e+04]]\n",
      "Y\n",
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[ : , 0] = labelencoder_X.fit_transform(X[ : , 0])\n",
    "#Creating a dummy variable\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "labelencoder_Y = LabelEncoder()\n",
    "Y =  labelencoder_Y.fit_transform(Y)\n",
    "print(\"---------------------\")\n",
    "print(\"Step 4: Encoding categorical data\")\n",
    "print(\"X\")\n",
    "print(X)\n",
    "print(\"Y\")\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第五步：拆分数据集为测试集合和训练集合\n",
    "把数据集拆分成两个：一个是用来训练模型的训练集合，另一个是用来验证模型的测试集合。两者比例一般是80:20。我们导入sklearn.crossvalidation库中的train_test_split()方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Step 5: Splitting the datasets into training sets and Test sets\n",
      "X_train\n",
      "[[  0.00000000e+00   1.00000000e+00   0.00000000e+00   4.00000000e+01\n",
      "    6.37777778e+04]\n",
      " [  1.00000000e+00   0.00000000e+00   0.00000000e+00   3.70000000e+01\n",
      "    6.70000000e+04]\n",
      " [  0.00000000e+00   0.00000000e+00   1.00000000e+00   2.70000000e+01\n",
      "    4.80000000e+04]\n",
      " [  0.00000000e+00   0.00000000e+00   1.00000000e+00   3.87777778e+01\n",
      "    5.20000000e+04]\n",
      " [  1.00000000e+00   0.00000000e+00   0.00000000e+00   4.80000000e+01\n",
      "    7.90000000e+04]\n",
      " [  0.00000000e+00   0.00000000e+00   1.00000000e+00   3.80000000e+01\n",
      "    6.10000000e+04]\n",
      " [  1.00000000e+00   0.00000000e+00   0.00000000e+00   4.40000000e+01\n",
      "    7.20000000e+04]\n",
      " [  1.00000000e+00   0.00000000e+00   0.00000000e+00   3.50000000e+01\n",
      "    5.80000000e+04]]\n",
      "X_test\n",
      "[[  0.00000000e+00   1.00000000e+00   0.00000000e+00   3.00000000e+01\n",
      "    5.40000000e+04]\n",
      " [  0.00000000e+00   1.00000000e+00   0.00000000e+00   5.00000000e+01\n",
      "    8.30000000e+04]]\n",
      "Y_train\n",
      "[1 1 1 0 1 0 0 1]\n",
      "Y_test\n",
      "[0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymao/usr/miniconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = 0.2, random_state = 0)\n",
    "print(\"---------------------\")\n",
    "print(\"Step 5: Splitting the datasets into training sets and Test sets\")\n",
    "print(\"X_train\")\n",
    "print(X_train)\n",
    "print(\"X_test\")\n",
    "print(X_test)\n",
    "print(\"Y_train\")\n",
    "print(Y_train)\n",
    "print(\"Y_test\")\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第六步：特征量化\n",
    "大部分模型算法使用两点间的欧氏距离表示，但此特征在幅度、单位和范围姿态问题上变化很大。在距离计算中，高幅度的特征比低幅度特征权重更大。可用特征标准化或Z值归一化解决。导入sklearn.preprocessing库的StandardScalar类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Step 6: Feature Scaling\n",
      "X_train\n",
      "[[-1.          2.64575131 -0.77459667  0.26306757  0.12381479]\n",
      " [ 1.         -0.37796447 -0.77459667 -0.25350148  0.46175632]\n",
      " [-1.         -0.37796447  1.29099445 -1.97539832 -1.53093341]\n",
      " [-1.         -0.37796447  1.29099445  0.05261351 -1.11141978]\n",
      " [ 1.         -0.37796447 -0.77459667  1.64058505  1.7202972 ]\n",
      " [-1.         -0.37796447  1.29099445 -0.0813118  -0.16751412]\n",
      " [ 1.         -0.37796447 -0.77459667  0.95182631  0.98614835]\n",
      " [ 1.         -0.37796447 -0.77459667 -0.59788085 -0.48214934]]\n",
      "X_test\n",
      "[[ 0.  0.  0. -1. -1.]\n",
      " [ 0.  0.  0.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.fit_transform(X_test)\n",
    "print(\"---------------------\")\n",
    "print(\"Step 6: Feature Scaling\")\n",
    "print(\"X_train\")\n",
    "print(X_train)\n",
    "print(\"X_test\")\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>完整的项目请前往Github项目<a href=\"https://github.com/MachineLearning100/100-Days-Of-ML-Code\">100-Days-Of-ML-Code</a>查看。有任何的建议或者意见欢迎在issue中提出~</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
